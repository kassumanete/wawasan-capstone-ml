{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Modelling\n",
    "This is a notebook to experiment with the data modelling of the sales quantity data.\n",
    "This was done on a cloud instance so the file paths will be different if you are running this locally.\n",
    "Note that the dataset is also propiertary so it will not be included in this repository."
   ],
   "metadata": {
    "collapsed": false,
    "id": "qdcabJugDpt3"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Imports and Constants\n",
    "We will be using the following libraries:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "k2UxcDF3DpuA"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "id": "aryZ62neeLK9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686516602281,
     "user_tz": -420,
     "elapsed": 297,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "WINDOW = 20\n",
    "BATCH_SIZE = 2048\n",
    "BUFFER = 100000"
   ],
   "metadata": {
    "id": "p8TxgaNyrSqD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686516602563,
     "user_tz": -420,
     "elapsed": 2,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zHaCTqUKeLK_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is a csv extracted from an SQL database and cleaned. It contains the following columns:\n",
    "- **date:** The date of the sale\n",
    "- **item_code:** The code of the product sold\n",
    "- **quantity:** The quantity of the product sold on that day"
   ],
   "metadata": {
    "collapsed": false,
    "id": "LoB4HRRYDpuG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# %pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GwN8M6XleqWL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686516606100,
     "user_tz": -420,
     "elapsed": 1979,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "36bb114a-5214-4038-d494-be5ee51ec088"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# filepath = 'sales_quantity.csv' #for local imports\n",
    "# filepath = '/home/jupyter/sales_quantity.csv' #vm-instance\n",
    "filepath = '/content/drive/MyDrive/Documents/uni_work/Bangkit2023/batch1/capstone/repo/ml_modeling/sales_quantity.csv' #colab\n",
    "data = pd.read_csv(filepath,names=['date','item_code','quantity'],\n",
    "                   dtype = {'item_code':str, 'quantity':np.float64},header = 0 )\n",
    "data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "lP40rLWweLLA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686516606795,
     "user_tz": -420,
     "elapsed": 322,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "26bfc6de-158c-43a1-8a0d-ca79760ef3e8"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform data\n",
    "We need to change the data so that it has the following format for training:\n",
    "- **Input:** [*The tokenized item code, day, month, day of the week, day of the year, {A sequence of 20 days of sales data for a particular product}*]\n",
    "- **Output:** The quantity sold for that product in the following day\n",
    ">**Note:**\n",
    "    - *The tokenized item code is the index of the item code in the tokenizer's word index.*\n",
    "    - *The day component of the date is the day of the month.*\n",
    "    - *The month component of the date is the month of the year.*\n",
    "    - *The day of the week is a number between 0 and 6, where 0 is Monday and 6 is Sunday.*\n",
    "    - *The day of the year is a number between 1 and 365, where January 1st is 1 and December 31st is 365.*\n",
    "    - *The sequence of 20 days of sales data is the window size we will use for training the model. The data will be normalized*\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "RtOebWGEDpuH"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#extract date features from date column\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "\n"
   ],
   "metadata": {
    "id": "RtmLJm2MeLLC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686516616447,
     "user_tz": -420,
     "elapsed": 668,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to create a wide dataframe with each item code as a column and the quantity sold for each day as the values."
   ],
   "metadata": {
    "collapsed": false,
    "id": "ZeE9zB4ADpuI"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#stack dataframe based on item_code\n",
    "item_sales = data.groupby(['item_code','date','year','month','day','day_of_week','day_of_year'])['quantity'].sum().unstack(level=0)\n",
    "#turn each NaN value to 0\n",
    "item_sales = item_sales.sort_values('date')\n",
    "item_sales = item_sales.fillna(0)\n",
    "item_sales.reset_index(inplace=True)\n",
    "item_sales"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2903
    },
    "id": "7xKluIgqeLLD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686518835039,
     "user_tz": -420,
     "elapsed": 1096,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "ccc3d800-1148-4792-8356-0a3c793f283d"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare item code and dates\n",
    "Since the dataset will use date and item code feature as input, to create an array of item code mapped to every date value"
   ],
   "metadata": {
    "id": "xsruJC6cGgjq"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#prepare the list of item codes\n",
    "\n",
    "items = np.array(item_sales.columns[6:])\n",
    "total_items = items.shape[0]\n",
    "print(items.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VUPgJxvrGfWG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686517991908,
     "user_tz": -420,
     "elapsed": 4,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "4bbc929c-0b6c-4431-addd-4599b4e18f51"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare the array of date_related features, since we will be windowing these features\n",
    "# we ignore the first few ones\n",
    "\n",
    "dates = np.array(item_sales[['month','day','day_of_week','day_of_year']][WINDOW:])\n",
    "\n",
    "#normalize for cyclic feature\n",
    "\n",
    "dates = np.sin(dates) + np.cos(dates)\n",
    "total_dates = dates.shape[0]\n",
    "print(dates.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Px-R080vNxL-",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686519772117,
     "user_tz": -420,
     "elapsed": 296,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "acf9423a-99fb-480b-8190-e8a85f26baab"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create numpy arrays for each repeated item and dates for later joining."
   ],
   "metadata": {
    "id": "17Nnk5MXO5Zr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "repeated_items = items.repeat(total_dates)\n",
    "repeated_dates = dates.reshape(1,dates.shape[0],dates.shape[1]).repeat(total_items,axis=0).reshape(-1,4)\n",
    "\n",
    "print(repeated_items.shape)\n",
    "print(repeated_dates.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qv60yRN6O4ka",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686519779555,
     "user_tz": -420,
     "elapsed": 305,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "9d30eca4-7913-4090-84cd-fe1c7ace8129"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the sales data to be windowed\n",
    "We need to create windows of the sales data corresponding to the dates. This will be used as input and output for the data later on."
   ],
   "metadata": {
    "id": "nzIftBcmS8BW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#transpose the sales quantity so dates are columns\n",
    "sales_quantity = np.array(item_sales.iloc[:,6:]).T\n",
    "#create the windows\n",
    "windowed = np.lib.stride_tricks.sliding_window_view(sales_quantity, WINDOW+1, axis=-1).reshape(-1,WINDOW+1)\n",
    "print(f'Shape of windowed data {windowed.shape}')\n",
    "\n",
    "#seperate the input and target\n",
    "sales_input = windowed[:,:-1]\n",
    "target = windowed[:,-1]\n",
    "print(f'Shape of windowed input {sales_input.shape}')\n",
    "print(f'Shape of target {target.shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Af4QeCoQS7WP",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686519970914,
     "user_tz": -420,
     "elapsed": 2196,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "8885c5ab-ffbb-455f-e18e-52d7552db211"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization\n",
    "We need to normalize the input and output of the sales data. \n",
    "We can normalize the output in place while using a normalization layer to normalize the input\n",
    "\n"
   ],
   "metadata": {
    "id": "uTddBZSFaJpw"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "id": "e-Lybmh6DpuK"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we will be passing the item code as a feature to the model, we need to tokenize it.\n",
    "We use a helper function to create a tokenizer and fit it on the item codes.\n",
    "This will be also be used to later decode the predictions of the model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "gY4zcruMDpuJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def create_vectorize(item_code):\n",
    "    \"\"\"\n",
    "    Create a tokenizer to tokenize item codes.\n",
    "\n",
    "    Args:\n",
    "        item_code (list or Series): List or Series containing item codes.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.preprocessing.text.Tokenizer: Tokenizer object fitted on item codes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a tokenizer with no filters and case-sensitive tokenization\n",
    "    vectorize_layer = tf.keras.layers.StringLookup()\n",
    "\n",
    "    # Fit the tokenizer on the item codes\n",
    "    vectorize_layer.adapt(item_code)\n",
    "\n",
    "    return vectorize_layer\n",
    "\n",
    "# Create a tokenizer using item codes from item_sales dataframe columns\n",
    "vectorize_layer = create_vectorize(tf.constant(item_sales.columns[6:]))\n",
    "\n",
    "# Get the length of the tokenizer's word index\n",
    "tokenizer_word_count = vectorize_layer.vocabulary_size()\n",
    "print(f'Tokenizer has {tokenizer_word_count} tokens')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epIOoC6PeLLI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686517990819,
     "user_tz": -420,
     "elapsed": 805,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "a158822e-dbf6-488d-c849-47b98d01250e"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After preparing the dataset, we try to fine tune the learning rate of the algorithm.\n",
    "We only use a sample of the training set to speed up the process."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T04:08:58.168373800Z",
     "start_time": "2023-06-11T04:08:58.124552200Z"
    },
    "id": "UQOjeOKDDpuK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#create a list of models based on given learning rates and optimizers\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Creates a list of models based on the learning rates and optimizers given.\n",
    "\n",
    "    Args:\n",
    "        learning_rate_array (list): A list of learning rates and optimizers to use for each model.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of models.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n",
    "                               strides=1,\n",
    "                               activation=\"relu\", padding=\"causal\",\n",
    "                               input_shape=[25, 1]),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n",
    "        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "hfgBpzQRDpuK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "train_sample = train_set.unbatch().shuffle(BUFFER).take(train_len//10).batch(BATCH_SIZE).prefetch(1)\n",
    "val_sample = val_set.unbatch().shuffle(BUFFER).take(val_len//10).batch(BATCH_SIZE).prefetch(1)\n",
    "train_sample"
   ],
   "metadata": {
    "id": "s0KHTXKXDpuL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#callback to tune the learning rate\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mape\"])\n",
    "\n",
    "history = model.fit(train_sample, epochs=100, callbacks=[lr_schedule],validation_data = val_sample, verbose=2)\n"
   ],
   "metadata": {
    "id": "yaTRDm9_DpuL"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the learning rate array\n",
    "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set the grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the loss in log scale\n",
    "plt.semilogx(lrs, history.history[\"loss\"])\n",
    "\n",
    "# Increase the tickmarks size\n",
    "plt.tick_params('both', length=10, width=1, which='both')\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n"
   ],
   "metadata": {
    "id": "vJZTTjAfDpuL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OdJmrXkkDpuM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We found that the best learning rate is 10e-5 (or 1e-4)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "XfikhMBkDpuM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Reset the states generated by keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = create_model()"
   ],
   "metadata": {
    "id": "3TzUUy2lDpuM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set the learning rate\n",
    "learning_rate = 1e-4\n",
    "#set the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "#set the callback to stop the training if the validation loss doesn't improve\n",
    "callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mape\"])\n",
    "\n",
    "history = model.fit(train_set, epochs=500, validation_data = val_set, verbose=2, callbacks=[callback])"
   ],
   "metadata": {
    "id": "Ou9tWQPpDpuN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_set)"
   ],
   "metadata": {
    "id": "ZE9ZwLS5DpuN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "2JI6l44VDpuN"
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
