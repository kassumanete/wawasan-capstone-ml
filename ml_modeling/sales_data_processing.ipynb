{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports and Constants"
   ],
   "metadata": {
    "collapsed": false,
    "id": "q62739y7eLK1"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "Yk99K1mBe3cJ"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 23:07:35.850161: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 23:07:36.704629: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-09 23:07:36.704740: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-09 23:07:36.704750: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import tensorboard\n",
    "import os"
   ],
   "metadata": {
    "id": "aryZ62neeLK9",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287225110,
     "user_tz": -420,
     "elapsed": 3548,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:34.176792100Z",
     "start_time": "2023-06-09T23:07:32.519852600Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "WINDOW = 20\n",
    "BATCH_SIZE = 32\n",
    "BUFFER = 100"
   ],
   "metadata": {
    "id": "p8TxgaNyrSqD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287225115,
     "user_tz": -420,
     "elapsed": 41,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:34.180893700Z",
     "start_time": "2023-06-09T23:07:34.178872Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "zHaCTqUKeLK_"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "GwN8M6XleqWL",
    "executionInfo": {
     "status": "error",
     "timestamp": 1686285433739,
     "user_tz": -420,
     "elapsed": 553,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "92a58a81-65a9-484d-ee8d-24da43af0de5",
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:34.188952900Z",
     "start_time": "2023-06-09T23:07:34.182871400Z"
    }
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         date      item_code  quantity\n0  2022-08-26       00001000        15\n1  2022-08-26       00000500        14\n2  2023-01-01  8991102380706        13\n3  2023-01-01  8991102381017        13\n4  2023-01-01  8886008101053        20",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>item_code</th>\n      <th>quantity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-08-26</td>\n      <td>00001000</td>\n      <td>15</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-08-26</td>\n      <td>00000500</td>\n      <td>14</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2023-01-01</td>\n      <td>8991102380706</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2023-01-01</td>\n      <td>8991102381017</td>\n      <td>13</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2023-01-01</td>\n      <td>8886008101053</td>\n      <td>20</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filepath = 'sales_quantity.csv' #for local imports\n",
    "filepath = '/home/mariefloco/sales_quantity.csv' #colab\n",
    "data = pd.read_csv(filepath,names=['date','item_code','quantity'],header = 0 )\n",
    "data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "lP40rLWweLLA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226767,
     "user_tz": -420,
     "elapsed": 1689,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "4b1e67e9-1f43-4e30-ed74-aaa1a0de72ab",
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:34.471966900Z",
     "start_time": "2023-06-09T23:07:34.208005700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Transform data"
   ],
   "metadata": {
    "id": "wsum8uaFeLLB",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226768,
     "user_tz": -420,
     "elapsed": 16,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:34.471966900Z",
     "start_time": "2023-06-09T23:07:34.462963100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#extract date features from date column\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "\n"
   ],
   "metadata": {
    "id": "RtmLJm2MeLLC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226768,
     "user_tz": -420,
     "elapsed": 15,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:34.861736Z",
     "start_time": "2023-06-09T23:07:34.466966700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "item_code       date  year  month  day  day_of_week  day_of_year  \\\n0         2022-01-03  2022      1    3            0            3   \n1         2022-01-04  2022      1    4            1            4   \n2         2022-01-05  2022      1    5            2            5   \n3         2022-01-06  2022      1    6            3            6   \n4         2022-01-07  2022      1    7            4            7   \n\nitem_code  (90)NA18210500154(91)2403  (90)NA18211207820(91)2410  00000001  \\\n0                                0.0                        0.0       0.0   \n1                                0.0                        0.0       0.0   \n2                                0.0                        0.0       0.0   \n3                                0.0                        0.0       0.0   \n4                                0.0                        0.0       0.0   \n\nitem_code  00000002  ...  CL000448327  CL000450943  COS LT  COSLT-228  \\\n0               0.0  ...          0.0          0.0     0.0        0.0   \n1               0.0  ...          0.0          0.0     0.0        0.0   \n2               0.0  ...          0.0          0.0     0.0        1.0   \n3               0.0  ...          0.0          0.0     0.0        0.0   \n4               0.0  ...          0.0          0.0     0.0        0.0   \n\nitem_code  EC0102190002  EC0102191301  EC0103190002  EC0106190101  MP-2203  \\\n0                   0.0           0.0           0.0           0.0      0.0   \n1                   0.0           0.0           0.0           0.0      0.0   \n2                   0.0           0.0           0.0           0.0      0.0   \n3                   0.0           0.0           0.0           0.0      0.0   \n4                   0.0           0.0           0.0           0.0      0.0   \n\nitem_code  SLM0958266  \n0                 0.0  \n1                 0.0  \n2                 0.0  \n3                 0.0  \n4                 0.0  \n\n[5 rows x 13923 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>item_code</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>day_of_week</th>\n      <th>day_of_year</th>\n      <th>(90)NA18210500154(91)2403</th>\n      <th>(90)NA18211207820(91)2410</th>\n      <th>00000001</th>\n      <th>00000002</th>\n      <th>...</th>\n      <th>CL000448327</th>\n      <th>CL000450943</th>\n      <th>COS LT</th>\n      <th>COSLT-228</th>\n      <th>EC0102190002</th>\n      <th>EC0102191301</th>\n      <th>EC0103190002</th>\n      <th>EC0106190101</th>\n      <th>MP-2203</th>\n      <th>SLM0958266</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-01-03</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-01-04</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-01-05</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-01-06</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-01-07</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 13923 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack dataframe based on item_code\n",
    "item_sales = data.groupby(['item_code','date','year','month','day','day_of_week','day_of_year'])['quantity'].sum().unstack(level=0)\n",
    "#turn each NaN value to 0\n",
    "item_sales = item_sales.fillna(0)\n",
    "item_sales.reset_index(inplace=True)\n",
    "item_sales.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "7xKluIgqeLLD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226769,
     "user_tz": -420,
     "elapsed": 16,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "a50777a9-4d54-4691-80ea-735c48a7de9b",
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:35.605067200Z",
     "start_time": "2023-06-09T23:07:34.870709700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has 13917 tokens\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def create_tokenizer(item_code):\n",
    "    \"\"\"\n",
    "    Create a tokenizer to tokenize item codes.\n",
    "\n",
    "    Args:\n",
    "        item_code (list or Series): List or Series containing item codes.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.preprocessing.text.Tokenizer: Tokenizer object fitted on item codes.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a tokenizer with no filters and case-sensitive tokenization\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='', lower=False)\n",
    "\n",
    "    # Fit the tokenizer on the item codes\n",
    "    tokenizer.fit_on_texts(item_code)\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "# Create a tokenizer using item codes from item_sales dataframe columns\n",
    "tokenizer = create_tokenizer(item_sales.columns[6:].str.replace(' ', ''))\n",
    "\n",
    "# Get the length of the tokenizer's word index\n",
    "tokenizer_word_count = len(tokenizer.word_index)\n",
    "\n",
    "print(f'Tokenizer has {tokenizer_word_count} tokens')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epIOoC6PeLLI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287226769,
     "user_tz": -420,
     "elapsed": 15,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "3a27cb81-ce45-4691-f648-7a5ca46f2095",
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:35.695285600Z",
     "start_time": "2023-06-09T23:07:35.638181100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 13917 numpy arrays with each one having shape (431, 5)\n",
      "[[ 1.          1.38177329 -1.37905342  0.68075479 -1.37905342]\n",
      " [ 1.          1.38177329 -0.48139935  1.         -0.48139935]\n",
      " [ 1.          1.38177329  0.85885106  1.38177329  0.85885106]\n",
      " ...\n",
      " [ 1.         -0.84887249 -0.83378017 -0.84887249  1.37024645]\n",
      " [ 1.         -0.84887249  0.51070471 -1.41044612  0.44592305]\n",
      " [ 1.         -1.41044612  1.38177329 -0.67526209 -0.88837995]]\n"
     ]
    }
   ],
   "source": [
    "# Extract date values from item_sales dataframe columns\n",
    "dates = np.array(item_sales[['month', 'day', 'day_of_week', 'day_of_year']])\n",
    "\n",
    "# Perform cyclic encoding on the date values\n",
    "dates_cyclic = np.sin(dates) + np.cos(dates)\n",
    "\n",
    "prefix_features = []\n",
    "\n",
    "# Iterate over each product in item_sales columns\n",
    "for product in item_sales.columns[6:].str.replace(' ', ''):\n",
    "    # Create prefix features for the product\n",
    "    prefix_feature = np.array([\n",
    "        [\n",
    "            tokenizer.word_index[product],\n",
    "            dates_cyclic[j][0],\n",
    "            dates_cyclic[j][1],\n",
    "            dates_cyclic[j][2],\n",
    "            dates_cyclic[j][3]\n",
    "        ]\n",
    "        for j in range(WINDOW, len(item_sales))\n",
    "    ], dtype=np.float64)\n",
    "    \n",
    "    prefix_features.append(prefix_feature)\n",
    "\n",
    "# Get the total number of prefix features arrays and the shape of the first array\n",
    "prefix_features_count = len(prefix_features)\n",
    "prefix_features_shape = prefix_features[0].shape\n",
    "\n",
    "print(f\"A total of {prefix_features_count} numpy arrays with each one having shape {prefix_features_shape}\")\n",
    "\n",
    "print(prefix_features[0])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojZPbyzjeLLE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287232903,
     "user_tz": -420,
     "elapsed": 6146,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "105b1cce-2c70-4202-8df4-0cd4b8bbe6d7",
    "ExecuteTime": {
     "end_time": "2023-06-09T23:07:42.845356900Z",
     "start_time": "2023-06-09T23:07:35.690286100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-09 23:07:48.155864: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:48.247473: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:48.249175: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:48.254898: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-09 23:07:48.257737: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:48.259405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:48.260998: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:49.789394: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:49.792122: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:49.793605: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-09 23:07:49.795609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    },
    {
     "data": {
      "text/plain": "TensorSpec(shape=(), dtype=tf.float64, name=None)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert each item sales to a tensorflow dataset\n",
    "sales_datasets = [tf.data.Dataset.from_tensor_slices(item_sales[column]) for column in item_sales.columns[7:]]\n",
    "sales_datasets[0].element_spec"
   ],
   "metadata": {
    "id": "G-yzQnnHeLLF",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287281623,
     "user_tz": -420,
     "elapsed": 23741,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "88bebf73-dd69-423a-9c9a-2f373ad9dba7",
    "ExecuteTime": {
     "end_time": "2023-06-09T23:08:08.546033200Z",
     "start_time": "2023-06-09T23:07:46.502330500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def window_dataset(token_time_ds, sales_ds, window_size):\n",
    "    \"\"\"\n",
    "    Create a windowed dataset by combining token_time_ds and sales_ds.\n",
    "\n",
    "    Args:\n",
    "        token_time_ds (tf.data.Dataset): Dataset containing token and time information.\n",
    "        sales_ds (tf.data.Dataset): Dataset containing sales information.\n",
    "        window_size (int): Size of the window for creating sequences.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: Windowed dataset with input-output pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create windows of size window_size+1, shifting by 1, and dropping any incomplete windows\n",
    "    sales_ds = sales_ds.window(window_size+1, shift=1, drop_remainder=True)\n",
    "\n",
    "    # Flatten the windows into individual datasets and combine them into a single dataset\n",
    "    sales_ds = sales_ds.flat_map(lambda w: w.batch(window_size+1))\n",
    "\n",
    "    # Concatenate token_time_ds and sales_ds tensors along the last axis\n",
    "    windowed_tensors = tf.concat((list(token_time_ds), list(sales_ds)), axis=-1)\n",
    "\n",
    "    # Create a new dataset from the concatenated tensors\n",
    "    ds = tf.data.Dataset.from_tensor_slices(windowed_tensors)\n",
    "\n",
    "    # Map each element of the dataset to input-output pairs\n",
    "    ds = ds.map(lambda x: (x[:-1], x[-1]))\n",
    "\n",
    "    return ds\n"
   ],
   "metadata": {
    "id": "R75X6KAVeLLG",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686287281624,
     "user_tz": -420,
     "elapsed": 25,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T23:08:08.555076300Z",
     "start_time": "2023-06-09T23:08:08.509153500Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "#window the dataset in batches\n",
    "ds = [window_dataset(prefix_features[i],sales_datasets[i], WINDOW) for i in range(len(sales_datasets))]\n",
    "del data\n",
    "del prefix_features\n",
    "del sales_datasets\n",
    "del item_sales"
   ],
   "metadata": {
    "id": "PuQNPgFAeLLH",
    "executionInfo": {
     "status": "error",
     "timestamp": 1686287808730,
     "user_tz": -420,
     "elapsed": 527130,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "outputId": "f1d60b1b-662b-44bf-8a0d-6476f0164d9b",
    "ExecuteTime": {
     "end_time": "2023-06-09T23:28:00.153366700Z",
     "start_time": "2023-06-09T23:08:08.510200800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_dataset_partitions_tf(ds, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=False, shuffle_size=1000):\n",
    "    \"\"\"\n",
    "    Splits a TensorFlow dataset into training, validation, and test partitions.\n",
    "\n",
    "    Args:\n",
    "        ds (tf.data.Dataset): The input dataset.\n",
    "        ds_size (int): The total size of the input dataset.\n",
    "        train_split (float, optional): The fraction of data to allocate for training. Defaults to 0.8.\n",
    "        val_split (float, optional): The fraction of data to allocate for validation. Defaults to 0.1.\n",
    "        test_split (float, optional): The fraction of data to allocate for testing. Defaults to 0.1.\n",
    "        shuffle (bool, optional): Whether to shuffle the training dataset. Defaults to True.\n",
    "        shuffle_size (int, optional): The buffer size used for shuffling. Defaults to 10000.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the training, validation, and test partitions of the dataset.\n",
    "    \"\"\"\n",
    "    assert (train_split + test_split + val_split) == 1, \"The sum of train_split, val_split, and test_split must be 1.\"\n",
    "    \n",
    "    \n",
    "    train_size = int(train_split * ds_size)\n",
    "    val_size = int(val_split * ds_size)\n",
    "    \n",
    "    if shuffle:\n",
    "      # Specify seed to always have the same split distribution between runs\n",
    "      train_ds = ds.take(train_size).shuffle(shuffle_size, seed=12)\n",
    "    else:\n",
    "      train_ds = ds.take(train_size)\n",
    "    val_ds = ds.skip(train_size).take(val_size)\n",
    "    test_ds = ds.skip(train_size).skip(val_size)\n",
    "    \n",
    "    return train_ds, val_ds, test_ds\n"
   ],
   "metadata": {
    "id": "0aqw4tpeaayW",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686287808733,
     "user_tz": -420,
     "elapsed": 16,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T23:28:00.161316400Z",
     "start_time": "2023-06-09T23:28:00.158369300Z"
    }
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#split the datasets to train, val, test\n",
    "ds = [get_dataset_partitions_tf(items, 431, shuffle_size=BUFFER) for items in ds]"
   ],
   "metadata": {
    "id": "fZwJAsF3ojX7",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686287808735,
     "user_tz": -420,
     "elapsed": 17,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-09T23:28:11.716735500Z",
     "start_time": "2023-06-09T23:28:00.164333900Z"
    }
   },
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#split into individual sets\n",
    "train_set = [ds[i][0] for i in range(len(ds))]\n",
    "val_set = [ds[i][1] for i in range(len(ds))]\n",
    "test_set = [ds[i][2]for i in range(len(ds))]"
   ],
   "metadata": {
    "id": "sjITHB8MyfPi",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686287808736,
     "user_tz": -420,
     "elapsed": 18,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-10T00:33:31.773919300Z",
     "start_time": "2023-06-10T00:33:31.752915100Z"
    }
   },
   "execution_count": 67,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#turn into tensors\n",
    "train_set = tf.data.experimental.from_list(train_set, name = 'train').flat_map(lambda x: x)\n",
    "val_set = tf.data.experimental.from_list(val_set, name = 'val').flat_map(lambda x: x)\n",
    "test_set = tf.data.experimental.from_list(test_set, name = 'test').flat_map(lambda x: x)\n",
    "#find the length of each set using map\n",
    "train_len = train_set.reduce(0, lambda x, _: x + 1).numpy()\n",
    "val_len = val_set.reduce(0, lambda x, _: x + 1).numpy()\n",
    "test_len = test_set.reduce(0, lambda x, _: x + 1).numpy()\n",
    "\n",
    "print(f\"Training set length: {train_len}\")\n",
    "print(f\"Validation set length: {val_len}\")\n",
    "print(f\"Test set length: {test_len}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-06-10T00:33:32.104798400Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#batch and prefetch the datasets\n",
    "train_set = train_set.shuffle(BUFFER).batch(BATCH_SIZE).prefetch(1)\n",
    "val_set = val_set.batch(BUFFER).prefetch(1)\n",
    "test_set = test_set.batch(BUFFER).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T23:38:56.656365800Z",
     "start_time": "2023-06-09T23:38:56.636107800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#save the datasets\n",
    "tf.data.Dataset.save(train_set, 'train_set')\n",
    "tf.data.Dataset.save(val_set, 'val_set')\n",
    "tf.data.Dataset.save(test_set, 'test_set')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-06-09T23:38:33.390536800Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Modelling"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "#create a list of models based on given learning rates and optimizers\n",
    "def create_model():\n",
    "    \"\"\"\n",
    "    Creates a list of models based on the learning rates and optimizers given.\n",
    "\n",
    "    Args:\n",
    "        learning_rate_array (list): A list of learning rates and optimizers to use for each model.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of models.\n",
    "    \"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Conv1D(filters=64, kernel_size=3,\n",
    "                               strides=1,\n",
    "                               activation=\"relu\", padding=\"causal\",\n",
    "                               input_shape=[25, 1]),\n",
    "        tf.keras.layers.LSTM(64, return_sequences=True),\n",
    "        tf.keras.layers.LSTM(64),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(1)\n",
    "    ])\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-09T23:55:42.292255600Z",
     "start_time": "2023-06-09T23:55:42.275017200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f09588dd990> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f09588dd990>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, _: x + 1\n",
      "\n",
      "Match 1:\n",
      "lambda x, _: 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f09588dd990> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f09588dd990>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, _: x + 1\n",
      "\n",
      "Match 1:\n",
      "lambda x, _: 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f09588dd990> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f09588dd990>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, _: x + 1\n",
      "\n",
      "Match 1:\n",
      "lambda x, _: 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f0958dc5cf0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f0958dc5cf0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, _: x + 1\n",
      "\n",
      "Match 1:\n",
      "lambda x, _: 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function <lambda> at 0x7f0958dc5cf0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f0958dc5cf0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, _: x + 1\n",
      "\n",
      "Match 1:\n",
      "lambda x, _: 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function <lambda> at 0x7f0958dc5cf0> and will run it as-is.\n",
      "Cause: could not parse the source code of <function <lambda> at 0x7f0958dc5cf0>: found multiple definitions with identical signatures at the location. This error may be avoided by defining each lambda on a single line and with unique argument names. The matching definitions were:\n",
      "Match 0:\n",
      "lambda x, _: x + 1\n",
      "\n",
      "Match 1:\n",
      "lambda x, _: 1\n",
      "\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m model \u001B[38;5;241m=\u001B[39m create_model()\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# train_sample = train_set.shuffle(BUFFER).take(len(train_set.map(lambda x: 1, num_parallel_calls=tf.data.experimental.AUTOTUNE).reduce(tf.constant(0), lambda x,_: x+1))//10)\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m \u001B[43mtrain_set\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmap\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m_\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_parallel_calls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexperimental\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mAUTOTUNE\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconstant\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m_\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m+\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:2807\u001B[0m, in \u001B[0;36mDatasetV2.reduce\u001B[0;34m(self, initial_state, reduce_func, name)\u001B[0m\n\u001B[1;32m   2803\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name:\n\u001B[1;32m   2804\u001B[0m   metadata\u001B[38;5;241m.\u001B[39mname \u001B[38;5;241m=\u001B[39m _validate_and_encode(name)\n\u001B[1;32m   2805\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m structure\u001B[38;5;241m.\u001B[39mfrom_compatible_tensor_list(\n\u001B[1;32m   2806\u001B[0m     state_structure,\n\u001B[0;32m-> 2807\u001B[0m     \u001B[43mgen_dataset_ops\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreduce_dataset\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2808\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_variant_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2809\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstructure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor_list\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_structure\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2810\u001B[0m \u001B[43m        \u001B[49m\u001B[43mreduce_func\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2811\u001B[0m \u001B[43m        \u001B[49m\u001B[43mf\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreduce_func\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2812\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstructure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_flat_tensor_shapes\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_structure\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2813\u001B[0m \u001B[43m        \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstructure\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_flat_tensor_types\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate_structure\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2814\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mSerializeToString\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:6399\u001B[0m, in \u001B[0;36mreduce_dataset\u001B[0;34m(input_dataset, initial_state, other_arguments, f, output_types, output_shapes, use_inter_op_parallelism, metadata, name)\u001B[0m\n\u001B[1;32m   6397\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tld\u001B[38;5;241m.\u001B[39mis_eager:\n\u001B[1;32m   6398\u001B[0m   \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 6399\u001B[0m     _result \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_FastPathExecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   6400\u001B[0m \u001B[43m      \u001B[49m\u001B[43m_ctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mReduceDataset\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minitial_state\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   6401\u001B[0m \u001B[43m      \u001B[49m\u001B[43mother_arguments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mf\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput_types\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_types\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   6402\u001B[0m \u001B[43m      \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moutput_shapes\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_shapes\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43muse_inter_op_parallelism\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   6403\u001B[0m \u001B[43m      \u001B[49m\u001B[43muse_inter_op_parallelism\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   6404\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[1;32m   6405\u001B[0m   \u001B[38;5;28;01mexcept\u001B[39;00m _core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "train_sample = train_set.shuffle(BUFFER).take(train_len//10).batch(BATCH_SIZE).prefetch(1)\n",
    "val_sample = val_set.shuffle(BUFFER).take(val_len//10).batch(BATCH_SIZE).prefetch(1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T00:13:52.308052300Z",
     "start_time": "2023-06-10T00:09:31.687873700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-10 00:00:50.857716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-06-10 00:00:54.748355: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f03bbffce80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-10 00:00:54.748411: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-06-10 00:00:54.797046: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-06-10 00:00:55.209424: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   5639/Unknown - 51s 7ms/step - loss: 0.3214 - mae: 0.3813"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[36], line 11\u001B[0m\n\u001B[1;32m      5\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mAdam(learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-8\u001B[39m)\n\u001B[1;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mHuber(),\n\u001B[1;32m      8\u001B[0m               optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[1;32m      9\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmae\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 11\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mlr_schedule\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mval_set\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1641\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1639\u001B[0m callbacks\u001B[38;5;241m.\u001B[39mon_epoch_begin(epoch)\n\u001B[1;32m   1640\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m-> 1641\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39msteps():\n\u001B[1;32m   1642\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1643\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1644\u001B[0m             epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1647\u001B[0m             _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1648\u001B[0m         ):\n\u001B[1;32m   1649\u001B[0m             callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/data_adapter.py:1371\u001B[0m, in \u001B[0;36mDataHandler.steps\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insufficient_data:  \u001B[38;5;66;03m# Set by `catch_stop_iteration`.\u001B[39;00m\n\u001B[1;32m   1370\u001B[0m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[0;32m-> 1371\u001B[0m original_spe \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitem()\n\u001B[1;32m   1372\u001B[0m can_run_full_execution \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1373\u001B[0m     original_spe \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1374\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferred_steps \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   1375\u001B[0m     \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferred_steps \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m original_spe\n\u001B[1;32m   1376\u001B[0m )\n\u001B[1;32m   1378\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m can_run_full_execution:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:638\u001B[0m, in \u001B[0;36mBaseResourceVariable.numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    637\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnumpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m--> 638\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mcontext\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecuting_eagerly\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m    639\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mread_value()\u001B[38;5;241m.\u001B[39mnumpy()\n\u001B[1;32m    640\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m    641\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy() is only available when eager execution is enabled.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:2203\u001B[0m, in \u001B[0;36mexecuting_eagerly\u001B[0;34m()\u001B[0m\n\u001B[1;32m   2149\u001B[0m \u001B[38;5;129m@tf_export\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexecuting_eagerly\u001B[39m\u001B[38;5;124m\"\u001B[39m, v1\u001B[38;5;241m=\u001B[39m[])\n\u001B[1;32m   2150\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mexecuting_eagerly\u001B[39m():\n\u001B[1;32m   2151\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Checks whether the current thread has eager execution enabled.\u001B[39;00m\n\u001B[1;32m   2152\u001B[0m \n\u001B[1;32m   2153\u001B[0m \u001B[38;5;124;03m  Eager execution is enabled by default and this API returns `True`\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   2201\u001B[0m \u001B[38;5;124;03m    `True` if the current thread has eager execution enabled.\u001B[39;00m\n\u001B[1;32m   2202\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m-> 2203\u001B[0m   ctx \u001B[38;5;241m=\u001B[39m \u001B[43mcontext_safe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2204\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m ctx \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m default_execution_mode \u001B[38;5;241m==\u001B[39m EAGER_MODE\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/context.py:2119\u001B[0m, in \u001B[0;36mcontext_safe\u001B[0;34m()\u001B[0m\n\u001B[1;32m   2115\u001B[0m     _create_context()\n\u001B[1;32m   2116\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _context\n\u001B[0;32m-> 2119\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontext_safe\u001B[39m():\n\u001B[1;32m   2120\u001B[0m \u001B[38;5;250m  \u001B[39m\u001B[38;5;124;03m\"\"\"Returns current context (or None if one hasn't been initialized).\"\"\"\u001B[39;00m\n\u001B[1;32m   2121\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m _context\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#callback to tune the learning rate\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-8)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "\n",
    "history = model.fit(train_sample, epochs=100, callbacks=[lr_schedule],validation_data = val_sample, verbose=1)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-10T00:01:34.625813200Z",
     "start_time": "2023-06-10T00:00:38.759951900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
