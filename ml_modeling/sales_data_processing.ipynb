{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Modelling\n",
    "This is a notebook to experiment with the data modelling of the sales quantity data.\n",
    "This was done on a cloud instance so the file paths will be different if you are running this locally.\n",
    "Note that the dataset is also propiertary so it will not be included in this repository."
   ],
   "metadata": {
    "collapsed": false,
    "id": "rGIoBAPKP3fO"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 1.Imports and Constants\n",
    "We will be using the following libraries:"
   ],
   "metadata": {
    "collapsed": false,
    "id": "volNP3ksP3fU"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 01:56:27.253213: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 01:56:28.126265: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-12 01:56:28.126367: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-06-12 01:56:28.126376: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "id": "o1wVdUrPP3fV",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533868111,
     "user_tz": -420,
     "elapsed": 3423,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:28.147040400Z",
     "start_time": "2023-06-12T01:56:26.457726200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "WINDOW = 20\n",
    "BATCH_SIZE = 2048\n",
    "BUFFER = 100000"
   ],
   "metadata": {
    "id": "gUnl9bQTP3fX",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533868112,
     "user_tz": -420,
     "elapsed": 10,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:28.151035500Z",
     "start_time": "2023-06-12T01:56:28.148042500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load data"
   ],
   "metadata": {
    "collapsed": false,
    "id": "naCHG3Y2P3fY"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data is a csv extracted from an SQL database and cleaned. It contains the following columns:\n",
    "- **date:** The date of the sale\n",
    "- **item_code:** The code of the product sold\n",
    "- **quantity:** The quantity of the product sold on that day"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Iw1O9ZhDP3fY"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "# %pwd"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V6MlIlPbP3fZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533884554,
     "user_tz": -420,
     "elapsed": 16451,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "ccc48f6e-c19f-45c0-cc64-d77c2d66ff01",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:28.153036Z",
     "start_time": "2023-06-12T01:56:28.149050700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "         date                  item_code  quantity\n0  2022-02-18  (90)NA18210500154(91)2403       1.0\n1  2022-02-19  (90)NA18210500154(91)2403       1.0\n2  2022-02-20  (90)NA18210500154(91)2403       1.0\n3  2022-03-03  (90)NA18210500154(91)2403       1.0\n4  2022-03-05  (90)NA18210500154(91)2403       1.0",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>date</th>\n      <th>item_code</th>\n      <th>quantity</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-02-18</td>\n      <td>(90)NA18210500154(91)2403</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-02-19</td>\n      <td>(90)NA18210500154(91)2403</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-02-20</td>\n      <td>(90)NA18210500154(91)2403</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-03-03</td>\n      <td>(90)NA18210500154(91)2403</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-03-05</td>\n      <td>(90)NA18210500154(91)2403</td>\n      <td>1.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# filepath = 'sales_quantity.csv' #for local imports\n",
    "# filepath = '/home/jupyter/data/sales_quantity.csv' #vm-instance\n",
    "filepath = '/content/drive/MyDrive/Documents/uni_work/Bangkit2023/batch1/capstone/repo/ml_modeling/sales_quantity.csv' #colab\n",
    "data = pd.read_csv(filepath,names=['date','item_code','quantity'],\n",
    "                   dtype = {'item_code':str, 'quantity':np.float64},header = 0 )\n",
    "data.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "XYb_1SAfP3fZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533886734,
     "user_tz": -420,
     "elapsed": 2183,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "8f5dd8f5-8653-4b00-e546-48ab7d13eaa7",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:28.415241Z",
     "start_time": "2023-06-12T01:56:28.174027400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transform data\n",
    "We need to change the data so that it has the following format for training:\n",
    "- **Input:** [*The tokenized item code, day, month, day of the week, day of the year, {A sequence of 20 days of sales data for a particular product}*]\n",
    "- **Output:** The quantity sold for that product in the following day\n",
    ">**Note:**\n",
    "    - *The tokenized item code is the index of the item code in the tokenizer's word index.*\n",
    "    - *The day component of the date is the day of the month.*\n",
    "    - *The month component of the date is the month of the year.*\n",
    "    - *The day of the week is a number between 0 and 6, where 0 is Monday and 6 is Sunday.*\n",
    "    - *The day of the year is a number between 1 and 365, where January 1st is 1 and December 31st is 365.*\n",
    "    - *The sequence of 20 days of sales data is the window size we will use for training the model. The data will be normalized*\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "vxpu3jyxP3fa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "#extract date features from date column\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "data['year'] = data['date'].dt.year\n",
    "data['month'] = data['date'].dt.month\n",
    "data['day'] = data['date'].dt.day\n",
    "data['day_of_week'] = data['date'].dt.dayofweek\n",
    "data['day_of_year'] = data['date'].dt.dayofyear\n",
    "\n"
   ],
   "metadata": {
    "id": "qk0iqFIyP3fa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533887231,
     "user_tz": -420,
     "elapsed": 501,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:28.853881600Z",
     "start_time": "2023-06-12T01:56:28.427235700Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We need to create a wide dataframe with each item code as a column and the quantity sold for each day as the values."
   ],
   "metadata": {
    "collapsed": false,
    "id": "Na5a_P3FP3fb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "item_code       date  year  month  day  day_of_week  day_of_year  \\\n0         2022-01-03  2022      1    3            0            3   \n1         2022-01-04  2022      1    4            1            4   \n2         2022-01-05  2022      1    5            2            5   \n3         2022-01-06  2022      1    6            3            6   \n4         2022-01-07  2022      1    7            4            7   \n\nitem_code  (90)NA18210500154(91)2403  (90)NA18211207820(91)2410  00000001  \\\n0                                0.0                        0.0       0.0   \n1                                0.0                        0.0       0.0   \n2                                0.0                        0.0       0.0   \n3                                0.0                        0.0       0.0   \n4                                0.0                        0.0       0.0   \n\nitem_code  00000002  ...  CL000448327  CL000450943  COS LT  COSLT-228  \\\n0               0.0  ...          0.0          0.0     0.0        0.0   \n1               0.0  ...          0.0          0.0     0.0        0.0   \n2               0.0  ...          0.0          0.0     0.0        1.0   \n3               0.0  ...          0.0          0.0     0.0        0.0   \n4               0.0  ...          0.0          0.0     0.0        0.0   \n\nitem_code  EC0102190002  EC0102191301  EC0103190002  EC0106190101  MP-2203  \\\n0                   0.0           0.0           0.0           0.0      0.0   \n1                   0.0           0.0           0.0           0.0      0.0   \n2                   0.0           0.0           0.0           0.0      0.0   \n3                   0.0           0.0           0.0           0.0      0.0   \n4                   0.0           0.0           0.0           0.0      0.0   \n\nitem_code  SLM0958266  \n0                 0.0  \n1                 0.0  \n2                 0.0  \n3                 0.0  \n4                 0.0  \n\n[5 rows x 13937 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>item_code</th>\n      <th>date</th>\n      <th>year</th>\n      <th>month</th>\n      <th>day</th>\n      <th>day_of_week</th>\n      <th>day_of_year</th>\n      <th>(90)NA18210500154(91)2403</th>\n      <th>(90)NA18211207820(91)2410</th>\n      <th>00000001</th>\n      <th>00000002</th>\n      <th>...</th>\n      <th>CL000448327</th>\n      <th>CL000450943</th>\n      <th>COS LT</th>\n      <th>COSLT-228</th>\n      <th>EC0102190002</th>\n      <th>EC0102191301</th>\n      <th>EC0103190002</th>\n      <th>EC0106190101</th>\n      <th>MP-2203</th>\n      <th>SLM0958266</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2022-01-03</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>3</td>\n      <td>0</td>\n      <td>3</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2022-01-04</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>4</td>\n      <td>1</td>\n      <td>4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2022-01-05</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>5</td>\n      <td>2</td>\n      <td>5</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2022-01-06</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>6</td>\n      <td>3</td>\n      <td>6</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2022-01-07</td>\n      <td>2022</td>\n      <td>1</td>\n      <td>7</td>\n      <td>4</td>\n      <td>7</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 13937 columns</p>\n</div>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stack dataframe based on item_code\n",
    "item_sales = data.groupby(['item_code','date','year','month','day','day_of_week','day_of_year'])['quantity'].sum().unstack(level=0)\n",
    "#turn each NaN value to 0\n",
    "item_sales = item_sales.sort_values('date')\n",
    "item_sales = item_sales.fillna(0)\n",
    "item_sales.reset_index(inplace=True)\n",
    "item_sales.head()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "F7AeXq2nP3fb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533888015,
     "user_tz": -420,
     "elapsed": 786,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "159648fc-3152-440f-a7c7-7fbf74e4989f",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:29.538901300Z",
     "start_time": "2023-06-12T01:56:28.878531100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare item code and dates\n",
    "Since the dataset will use date and item code feature as input, to create an array of item code mapped to every date value"
   ],
   "metadata": {
    "collapsed": false,
    "id": "YtNRBhlpP3fb"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13931,)\n"
     ]
    }
   ],
   "source": [
    "#prepare the list of item codes\n",
    "\n",
    "items = np.array(item_sales.columns[6:])\n",
    "total_items = items.shape[0]\n",
    "print(items.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQRxvzHdP3fb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533888016,
     "user_tz": -420,
     "elapsed": 12,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "4112e120-103a-4c90-837b-d124acafac1a",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:29.571903300Z",
     "start_time": "2023-06-12T01:56:29.528903Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(431, 4)\n"
     ]
    }
   ],
   "source": [
    "# prepare the array of date_related features, since we will be windowing these features\n",
    "# we ignore the first few ones\n",
    "\n",
    "dates = np.array(item_sales[['month','day','day_of_week','day_of_year']][WINDOW:])\n",
    "\n",
    "#normalize for cyclic feature\n",
    "\n",
    "dates = np.sin(dates) + np.cos(dates)\n",
    "total_dates = dates.shape[0]\n",
    "print(dates.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drMkz4LqP3fc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533888016,
     "user_tz": -420,
     "elapsed": 10,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "bf2a5324-2f40-414c-dfcb-d6b8b5b6a4f3",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:29.571903300Z",
     "start_time": "2023-06-12T01:56:29.556918200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create numpy arrays for each repeated item and dates for later joining."
   ],
   "metadata": {
    "collapsed": false,
    "id": "W94gxsYDP3fd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6004261,)\n",
      "(6004261, 4)\n"
     ]
    }
   ],
   "source": [
    "repeated_items = items.repeat(total_dates)\n",
    "repeated_dates = dates.reshape(1,dates.shape[0],dates.shape[1]).repeat(total_items,axis=0).reshape(-1,4)\n",
    "\n",
    "print(repeated_items.shape)\n",
    "print(repeated_dates.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TXd5gwouP3fd",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533888017,
     "user_tz": -420,
     "elapsed": 8,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "68611638-c31e-42d2-c9ee-890a6e4a155a",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:29.748936300Z",
     "start_time": "2023-06-12T01:56:29.560913500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Prepare the sales data to be windowed\n",
    "We need to create windows of the sales data corresponding to the dates. This will be used as input and output for the data later on."
   ],
   "metadata": {
    "collapsed": false,
    "id": "qpqb-6i9P3fd"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of windowed data (6004261, 21)\n",
      "Shape of windowed input (6004261, 20)\n",
      "Shape of target (6004261,)\n"
     ]
    }
   ],
   "source": [
    "#transpose the sales quantity so dates are columns\n",
    "sales_quantity = np.array(item_sales.iloc[:,6:]).T\n",
    "#create the windows\n",
    "windowed = np.lib.stride_tricks.sliding_window_view(sales_quantity, WINDOW+1, axis=-1).reshape(-1,WINDOW+1)\n",
    "print(f'Shape of windowed data {windowed.shape}')\n",
    "\n",
    "#seperate the input and target\n",
    "sales_input = windowed[:,:-1]\n",
    "target = windowed[:,-1]\n",
    "print(f'Shape of windowed input {sales_input.shape}')\n",
    "print(f'Shape of target {target.shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XeeqjL62P3fe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533888541,
     "user_tz": -420,
     "elapsed": 529,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "edf2d2aa-8454-46c3-f668-f5d8e5446888",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:30.099944400Z",
     "start_time": "2023-06-12T01:56:29.652896300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Normalization\n",
    "We need to normalize the input and output of the sales data.\n",
    "We can normalize the output in place while using a normalization layer to normalize the input\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "id": "lE1JsHaiP3fe"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def create_normalization_layer(data):\n",
    "    \"\"\"\n",
    "    Create a normalization layer to normalize the input data.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.layers.Normalization: Normalization layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a normalization layer\n",
    "    normalization_layer = tf.keras.layers.Normalization()\n",
    "\n",
    "    # Fit the layer on the input data\n",
    "    normalization_layer.adapt(data)\n",
    "\n",
    "    return normalization_layer"
   ],
   "metadata": {
    "id": "2VHe65MHP3fe",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686533888542,
     "user_tz": -420,
     "elapsed": 5,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-12T01:56:30.100953300Z",
     "start_time": "2023-06-12T01:56:30.097968300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 01:56:32.812196: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:32.824440: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:32.826107: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:32.828595: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-12 01:56:32.830609: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:32.832277: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:32.833892: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:33.616191: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:33.618112: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:33.619783: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:981] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-12 01:56:33.621392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13582 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# Create a normalization layer for the input\n",
    "normalization_layer = create_normalization_layer(sales_input)\n",
    "\n",
    "# Normalize the output\n",
    "target_norm = np.linalg.norm(target)\n",
    "target = target/target_norm\n"
   ],
   "metadata": {
    "id": "KG5pDVt5P3ff",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686534044498,
     "user_tz": -420,
     "elapsed": 155961,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:53.925906700Z",
     "start_time": "2023-06-12T01:56:30.101946400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we will be passing the item code as a feature to the model, we need to tokenize it.\n",
    "We use a helper function to create a tokenizer and fit it on the item codes.\n",
    "This will be also be used to later decode the predictions of the model."
   ],
   "metadata": {
    "collapsed": false,
    "id": "4Ir7k5HRP3ff"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer has 13932 tokens\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer using item codes from item_sales dataframe columns\n",
    "vectorize_layer = tf.keras.layers.StringLookup(vocabulary=items)\n",
    "\n",
    "# Get the length of the tokenizer's word index\n",
    "tokenizer_word_count = vectorize_layer.vocabulary_size()\n",
    "print(f'Tokenizer has {tokenizer_word_count} tokens')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qi9t1Ao8P3fg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686534044498,
     "user_tz": -420,
     "elapsed": 14,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "0c53b210-7ee3-4e28-8755-55074d068c39",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:53.978971500Z",
     "start_time": "2023-06-12T01:59:53.929903900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting Data\n",
    "We split the data into training, validation and test sets. We use the first 80% of the data for training, the next 10% for validation and the last 10% for testing. The best way to do this is to use the most recent data for testing and validation since it is more representative of the future."
   ],
   "metadata": {
    "collapsed": false,
    "id": "l025Ku9KP3fg"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: (612964, 1), (612964, 4), (612964, 20), (612964, 1)\n",
      "Validation: (599033, 1), (599033, 4), (599033, 20), (599033, 1)\n",
      "Train: (4792264, 1), (4792264, 4), (4792264, 20), (4792264, 1)\n"
     ]
    }
   ],
   "source": [
    "#split the data into train, validation and test sets\n",
    "train_split = int(0.8 * total_dates)\n",
    "val_split = int(0.9 * total_dates)\n",
    "\n",
    "#reshape the data to be 3D to easily extract the splits\n",
    "repeated_items = repeated_items.reshape(total_dates,-1,1)\n",
    "repeated_dates = repeated_dates.reshape(total_dates,-1,4)\n",
    "sales_input = sales_input.reshape(total_dates,-1,WINDOW)\n",
    "target = target.reshape(total_dates,-1,1)\n",
    "\n",
    "#split the data\n",
    "train_items = repeated_items[:train_split].reshape(-1,1)\n",
    "train_dates = repeated_dates[:train_split].reshape(-1,4)\n",
    "train_sales = sales_input[:train_split].reshape(-1,WINDOW)\n",
    "train_target = target[:train_split].reshape(-1,1)\n",
    "\n",
    "val_items = repeated_items[train_split:val_split].reshape(-1,1)\n",
    "val_dates = repeated_dates[train_split:val_split].reshape(-1,4)\n",
    "val_sales = sales_input[train_split:val_split].reshape(-1,WINDOW)\n",
    "val_target = target[train_split:val_split].reshape(-1,1)\n",
    "\n",
    "test_items = repeated_items[val_split:].reshape(-1,1)\n",
    "test_dates = repeated_dates[val_split:].reshape(-1,4)\n",
    "test_sales = sales_input[val_split:].reshape(-1,WINDOW)\n",
    "test_target = target[val_split:].reshape(-1,1)\n",
    "\n",
    "train_length = train_items.shape[0]\n",
    "val_length = val_items.shape[0]\n",
    "test_length = test_items.shape[0]\n",
    "\n",
    "#ensure all the splits are of the same shape\n",
    "print(f'Test: {test_items.shape}, {test_dates.shape}, {test_sales.shape}, {test_target.shape}')\n",
    "print(f'Validation: {val_items.shape}, {val_dates.shape}, {val_sales.shape}, {val_target.shape}')\n",
    "print(f'Train: {train_items.shape}, {train_dates.shape}, {train_sales.shape}, {train_target.shape}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KuWU_dUmP3fg",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686534044499,
     "user_tz": -420,
     "elapsed": 12,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "f3f44c38-b1a5-4e67-9cec-3bf5237b93f5",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:53.980010300Z",
     "start_time": "2023-06-12T01:59:53.970010800Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: ((TensorSpec(shape=(1,), dtype=tf.int64, name=None), TensorSpec(shape=(4,), dtype=tf.float64, name=None), TensorSpec(shape=(20,), dtype=tf.float64, name=None)), TensorSpec(shape=(1,), dtype=tf.float64, name=None))\n",
      "Validation: ((TensorSpec(shape=(1,), dtype=tf.int64, name=None), TensorSpec(shape=(4,), dtype=tf.float64, name=None), TensorSpec(shape=(20,), dtype=tf.float64, name=None)), TensorSpec(shape=(1,), dtype=tf.float64, name=None))\n",
      "Train: ((TensorSpec(shape=(1,), dtype=tf.int64, name=None), TensorSpec(shape=(4,), dtype=tf.float64, name=None), TensorSpec(shape=(20,), dtype=tf.float64, name=None)), TensorSpec(shape=(1,), dtype=tf.float64, name=None))\n"
     ]
    }
   ],
   "source": [
    "#Turn each split into a tensorflow dataset\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(((vectorize_layer(train_items), train_dates, train_sales), train_target))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(((vectorize_layer(val_items), val_dates, val_sales), val_target))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(((vectorize_layer(test_items), test_dates, test_sales), test_target))\n",
    "#show the shapes of the datasets\n",
    "print(f'Test: {test_dataset.element_spec}')\n",
    "print(f'Validation: {val_dataset.element_spec}')\n",
    "print(f'Train: {train_dataset.element_spec}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l9u0kNpMP3fh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686534045991,
     "user_tz": -420,
     "elapsed": 1498,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "2ee0531b-da96-4b6c-a01c-2409cb7091ac",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:56.037592900Z",
     "start_time": "2023-06-12T01:59:53.976981300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Analysis"
   ],
   "metadata": {
    "collapsed": false,
    "id": "Z3efqjVkP3fh"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model architecture\n",
    "Since we are using the different types of features each with their own importance we will not be using a Sequential model but attempt to use the keras Functional API to define the model."
   ],
   "metadata": {
    "id": "fMKgvaWeidpa"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "def create_model(vectorizer, normalizer):\n",
    "    '''\n",
    "    Create a model using the keras Functional API.\n",
    "\n",
    "    :return: keras model\n",
    "    '''\n",
    "    #input layers\n",
    "    item_input = tf.keras.Input(shape=(1,), name='item_input')\n",
    "    date_input = tf.keras.Input(shape=(4,), name='date_input')\n",
    "    sales_input = tf.keras.Input(shape=(WINDOW,),name='sales_input')\n",
    "\n",
    "    #embedding layers\n",
    "    item_feature = tf.keras.layers.Embedding(vectorizer.vocabulary_size(), 64)(item_input)\n",
    "    item_feature = tf.keras.layers.Flatten()(item_feature)\n",
    "\n",
    "    #date layers\n",
    "    date_feature = tf.keras.layers.Dense(32, activation='relu')(date_input)\n",
    "    date_feature = tf.keras.layers.Dense(16, activation='relu')(date_feature)\n",
    "    date_feature = tf.keras.layers.Dense(8, activation='relu')(date_feature)\n",
    "\n",
    "    #sales layers\n",
    "    sales_feature = normalizer(sales_input)\n",
    "    sales_feature = tf.keras.layers.Reshape((20,1))(sales_feature)\n",
    "    sales_feature = tf.keras.layers.Conv1D(32, 3, activation='relu', input_shape=(WINDOW,1))(sales_feature)\n",
    "    sales_feature = tf.keras.layers.LSTM(64, return_sequences=True)(sales_feature)\n",
    "    sales_feature = tf.keras.layers.LSTM(64)(sales_feature)\n",
    "\n",
    "\n",
    "    #concatenate all layers\n",
    "    x = tf.keras.layers.Concatenate()([item_feature, date_feature])\n",
    "    x = tf.keras.layers.Concatenate()([x, sales_feature])\n",
    "    x = tf.keras.layers.Dense(64, activation='relu')(x)\n",
    "    x = tf.keras.layers.Dense(32, activation='relu')(x)\n",
    "    output = tf.keras.layers.Dense(1)(x)\n",
    "\n",
    "    #create the model\n",
    "    model = tf.keras.Model(inputs=[item_input, date_input, sales_input], outputs=output)\n",
    "\n",
    "    return model\n"
   ],
   "metadata": {
    "id": "pwFrHrfaP3fh",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686534668837,
     "user_tz": -420,
     "elapsed": 3,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:56.044561900Z",
     "start_time": "2023-06-12T01:59:56.042562200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "After preparing the dataset, we try to fine tune the learning rate of the algorithm.\n",
    "We only use a sample of the training set to speed up the process."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-06-11T04:08:58.168373800Z",
     "start_time": "2023-06-11T04:08:58.124552200Z"
    },
    "id": "UQOjeOKDDpuK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " sales_input (InputLayer)       [(None, 20)]         0           []                               \n",
      "                                                                                                  \n",
      " date_input (InputLayer)        [(None, 4)]          0           []                               \n",
      "                                                                                                  \n",
      " normalization (Normalization)  (None, 20)           41          ['sales_input[0][0]']            \n",
      "                                                                                                  \n",
      " item_input (InputLayer)        [(None, 1)]          0           []                               \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 32)           160         ['date_input[0][0]']             \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 20, 1)        0           ['normalization[0][0]']          \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 1, 64)        891648      ['item_input[0][0]']             \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 16)           528         ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)                (None, 18, 32)       128         ['reshape[0][0]']                \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 64)           0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 8)            136         ['dense_1[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 18, 64)       24832       ['conv1d[0][0]']                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 72)           0           ['flatten[0][0]',                \n",
      "                                                                  'dense_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 64)           33024       ['lstm[0][0]']                   \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 136)          0           ['concatenate[0][0]',            \n",
      "                                                                  'lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      " dense_3 (Dense)                (None, 64)           8768        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           2080        ['dense_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 1)            33          ['dense_4[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 961,378\n",
      "Trainable params: 961,337\n",
      "Non-trainable params: 41\n",
      "__________________________________________________________________________________________________\n",
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vectorize_layer, normalization_layer)\n",
    "model.summary()\n",
    "tf.keras.utils.plot_model(model)"
   ],
   "metadata": {
    "id": "s0KHTXKXDpuL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686534682576,
     "user_tz": -420,
     "elapsed": 1144,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "4e483c2d-68e4-4f44-bd68-45274d1bd6d5",
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:56.894436100Z",
     "start_time": "2023-06-12T01:59:56.047560Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "#take a sample of the training set\n",
    "train_sample = train_dataset.shuffle(BUFFER).batch(BATCH_SIZE).take(train_length//10).prefetch(1)\n",
    "val_sample = val_dataset.batch(BATCH_SIZE).take(val_length//10).prefetch(1)"
   ],
   "metadata": {
    "id": "u5G_Eu-OP3fi",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1686534694876,
     "user_tz": -420,
     "elapsed": 295,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "ExecuteTime": {
     "end_time": "2023-06-12T01:59:56.899429800Z",
     "start_time": "2023-06-12T01:59:56.893431900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-12 02:00:02.487213: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:428] Loaded cuDNN version 8200\n",
      "2023-06-12 02:00:03.477570: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x7f03a0183f60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-06-12 02:00:03.477605: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
      "2023-06-12 02:00:03.552526: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2340/2340 [==============================] - 59s 22ms/step - loss: 0.0012 - mape: 37904272.0000 - val_loss: 0.0011 - val_mape: 35359412.0000 - lr: 1.0000e-08\n",
      "Epoch 2/100\n",
      "2340/2340 [==============================] - 42s 18ms/step - loss: 0.0012 - mape: 37885348.0000 - val_loss: 0.0011 - val_mape: 35340836.0000 - lr: 1.1220e-08\n",
      "Epoch 3/100\n",
      "2340/2340 [==============================] - 41s 17ms/step - loss: 0.0012 - mape: 37864144.0000 - val_loss: 0.0011 - val_mape: 35320024.0000 - lr: 1.2589e-08\n",
      "Epoch 4/100\n",
      "1231/2340 [==============>...............] - ETA: 17s - loss: 0.0012 - mape: 38085580.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[19], line 11\u001B[0m\n\u001B[1;32m      5\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39moptimizers\u001B[38;5;241m.\u001B[39mSGD(momentum\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.9\u001B[39m)\n\u001B[1;32m      7\u001B[0m model\u001B[38;5;241m.\u001B[39mcompile(loss\u001B[38;5;241m=\u001B[39mtf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mlosses\u001B[38;5;241m.\u001B[39mHuber(),\n\u001B[1;32m      8\u001B[0m               optimizer\u001B[38;5;241m=\u001B[39moptimizer,\n\u001B[1;32m      9\u001B[0m               metrics\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmape\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m---> 11\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mlr_schedule\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mval_sample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mverbose\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/keras/utils/traceback_utils.py:65\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     63\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 65\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     67\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/keras/engine/training.py:1650\u001B[0m, in \u001B[0;36mModel.fit\u001B[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001B[0m\n\u001B[1;32m   1642\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m tf\u001B[38;5;241m.\u001B[39mprofiler\u001B[38;5;241m.\u001B[39mexperimental\u001B[38;5;241m.\u001B[39mTrace(\n\u001B[1;32m   1643\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   1644\u001B[0m     epoch_num\u001B[38;5;241m=\u001B[39mepoch,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     _r\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[1;32m   1648\u001B[0m ):\n\u001B[1;32m   1649\u001B[0m     callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_begin(step)\n\u001B[0;32m-> 1650\u001B[0m     tmp_logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43miterator\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1651\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39mshould_sync:\n\u001B[1;32m   1652\u001B[0m         context\u001B[38;5;241m.\u001B[39masync_wait()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py:150\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 150\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    152\u001B[0m   filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001B[0m, in \u001B[0;36mFunction.__call__\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    877\u001B[0m compiler \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxla\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnonXla\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    879\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m OptionalXlaContext(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jit_compile):\n\u001B[0;32m--> 880\u001B[0m   result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    882\u001B[0m new_tracing_count \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mexperimental_get_tracing_count()\n\u001B[1;32m    883\u001B[0m without_tracing \u001B[38;5;241m=\u001B[39m (tracing_count \u001B[38;5;241m==\u001B[39m new_tracing_count)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001B[0m, in \u001B[0;36mFunction._call\u001B[0;34m(self, *args, **kwds)\u001B[0m\n\u001B[1;32m    909\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m    910\u001B[0m   \u001B[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001B[39;00m\n\u001B[1;32m    911\u001B[0m   \u001B[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001B[39;00m\n\u001B[0;32m--> 912\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_no_variable_creation_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwds\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=not-callable\u001B[39;00m\n\u001B[1;32m    913\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_variable_creation_fn \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    914\u001B[0m   \u001B[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001B[39;00m\n\u001B[1;32m    915\u001B[0m   \u001B[38;5;66;03m# in parallel.\u001B[39;00m\n\u001B[1;32m    916\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock\u001B[38;5;241m.\u001B[39mrelease()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001B[0m, in \u001B[0;36mTracingCompiler.__call__\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_lock:\n\u001B[1;32m    132\u001B[0m   (concrete_function,\n\u001B[1;32m    133\u001B[0m    filtered_flat_args) \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_define_function(args, kwargs)\n\u001B[0;32m--> 134\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_flat\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfiltered_flat_args\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcaptured_inputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconcrete_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcaptured_inputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001B[0m, in \u001B[0;36mConcreteFunction._call_flat\u001B[0;34m(self, args, captured_inputs, cancellation_manager)\u001B[0m\n\u001B[1;32m   1741\u001B[0m possible_gradient_type \u001B[38;5;241m=\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPossibleTapeGradientTypes(args)\n\u001B[1;32m   1742\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (possible_gradient_type \u001B[38;5;241m==\u001B[39m gradients_util\u001B[38;5;241m.\u001B[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001B[1;32m   1743\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m executing_eagerly):\n\u001B[1;32m   1744\u001B[0m   \u001B[38;5;66;03m# No tape is watching; skip to running the function.\u001B[39;00m\n\u001B[0;32m-> 1745\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_build_call_outputs(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_inference_function\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcall\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1746\u001B[0m \u001B[43m      \u001B[49m\u001B[43mctx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcancellation_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcancellation_manager\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1747\u001B[0m forward_backward \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_select_forward_and_backward_functions(\n\u001B[1;32m   1748\u001B[0m     args,\n\u001B[1;32m   1749\u001B[0m     possible_gradient_type,\n\u001B[1;32m   1750\u001B[0m     executing_eagerly)\n\u001B[1;32m   1751\u001B[0m forward_function, args_with_tangents \u001B[38;5;241m=\u001B[39m forward_backward\u001B[38;5;241m.\u001B[39mforward()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001B[0m, in \u001B[0;36m_EagerDefinedFunction.call\u001B[0;34m(self, ctx, args, cancellation_manager)\u001B[0m\n\u001B[1;32m    376\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _InterpolateFunctionError(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    377\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m cancellation_manager \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 378\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[43mexecute\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mexecute\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    379\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;28;43mstr\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msignature\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    380\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_num_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    381\u001B[0m \u001B[43m        \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    382\u001B[0m \u001B[43m        \u001B[49m\u001B[43mattrs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    383\u001B[0m \u001B[43m        \u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mctx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    384\u001B[0m   \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    385\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m execute\u001B[38;5;241m.\u001B[39mexecute_with_cancellation(\n\u001B[1;32m    386\u001B[0m         \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msignature\u001B[38;5;241m.\u001B[39mname),\n\u001B[1;32m    387\u001B[0m         num_outputs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_outputs,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    390\u001B[0m         ctx\u001B[38;5;241m=\u001B[39mctx,\n\u001B[1;32m    391\u001B[0m         cancellation_manager\u001B[38;5;241m=\u001B[39mcancellation_manager)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001B[0m, in \u001B[0;36mquick_execute\u001B[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001B[0m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m   ctx\u001B[38;5;241m.\u001B[39mensure_initialized()\n\u001B[0;32m---> 52\u001B[0m   tensors \u001B[38;5;241m=\u001B[39m \u001B[43mpywrap_tfe\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTFE_Py_Execute\u001B[49m\u001B[43m(\u001B[49m\u001B[43mctx\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_handle\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mop_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                      \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattrs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_outputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     55\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "#callback to tune the learning rate\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(momentum=0.9)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mape\"])\n",
    "\n",
    "history = model.fit(train_sample, epochs=100, callbacks=[lr_schedule],validation_data = val_sample, verbose=1)\n"
   ],
   "metadata": {
    "id": "yaTRDm9_DpuL",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 380
    },
    "executionInfo": {
     "status": "error",
     "timestamp": 1686534850109,
     "user_tz": -420,
     "elapsed": 28478,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    },
    "outputId": "3f3cb821-6cb3-4e70-8391-fa9f494bb0d0",
    "ExecuteTime": {
     "end_time": "2023-06-12T02:02:39.396457900Z",
     "start_time": "2023-06-12T01:59:56.894436100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the learning rate array\n",
    "lrs = 1e-8 * (10 ** (np.arange(100) / 20))\n",
    "\n",
    "# Set the figure size\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set the grid\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot the loss in log scale\n",
    "plt.semilogx(lrs, history.history[\"loss\"])\n",
    "\n",
    "# Increase the tickmarks size\n",
    "plt.tick_params('both', length=10, width=1, which='both')\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n"
   ],
   "metadata": {
    "id": "vJZTTjAfDpuL",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686534046303,
     "user_tz": -420,
     "elapsed": 11,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training"
   ],
   "metadata": {
    "collapsed": false,
    "id": "OdJmrXkkDpuM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We found that the best learning rate is 10e-5 (or 1e-4)."
   ],
   "metadata": {
    "collapsed": false,
    "id": "XfikhMBkDpuM"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Reset the states generated by keras\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "model = create_model()"
   ],
   "metadata": {
    "id": "3TzUUy2lDpuM",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686534046303,
     "user_tz": -420,
     "elapsed": 11,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#set the learning rate\n",
    "learning_rate = 1e-4\n",
    "#set the optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "\n",
    "#set the callback to stop the training if the validation loss doesn't improve\n",
    "callback = tf.keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True)\n",
    "\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mape\"])\n",
    "\n",
    "history = model.fit(train_set, epochs=500, validation_data = val_set, verbose=2, callbacks=[callback])"
   ],
   "metadata": {
    "id": "Ou9tWQPpDpuN",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686534046304,
     "user_tz": -420,
     "elapsed": 11,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation = model.evaluate(test_set)"
   ],
   "metadata": {
    "id": "ZE9ZwLS5DpuN",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686534046304,
     "user_tz": -420,
     "elapsed": 11,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "id": "2JI6l44VDpuN",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1686534046305,
     "user_tz": -420,
     "elapsed": 11,
     "user": {
      "displayName": "Muhammad Arief",
      "userId": "01585842397766244027"
     }
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  },
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "T4"
  },
  "accelerator": "TPU"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
